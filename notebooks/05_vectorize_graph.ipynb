{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce71877-46b6-474b-ab13-1f0eab4af318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import grpc\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lancedb\n",
    "from senzing import SzEngine, SzError\n",
    "from senzing_grpc import SzAbstractFactoryGrpc\n",
    "from senzing import SzEngineFlags\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0a3a0-63d9-40c5-a2cf-a234fc0d8ce1",
   "metadata": {},
   "source": [
    "## Connect to Senzing\n",
    "\n",
    "Opens a gRPC connection to the Senzing engine using environment variables.  We need this to export the fully resolved entity data that we will turn into graph nodes and vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d0712c-1e48-43ed-ba96-7875c3947597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Senzing at senzing:8261\n",
      "Connected to Senzing successfully\n"
     ]
    }
   ],
   "source": [
    "SENZING_HOST = os.getenv('SENZING_GRPC_HOST', 'senzing')\n",
    "SENZING_PORT = os.getenv('SENZING_GRPC_PORT', '8261')\n",
    "\n",
    "print(f\"Connecting to Senzing at {SENZING_HOST}:{SENZING_PORT}\")\n",
    "\n",
    "# Create gRPC channel and engine\n",
    "grpc_url = f\"{SENZING_HOST}:{SENZING_PORT}\"\n",
    "grpc_channel = grpc.insecure_channel(grpc_url)\n",
    "sz_abstract_factory = SzAbstractFactoryGrpc(grpc_channel)\n",
    "sz_engine = sz_abstract_factory.create_engine()\n",
    "\n",
    "print(\"Connected to Senzing successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928e336-6136-42ba-b420-e0b68e76646a",
   "metadata": {},
   "source": [
    "## Export All Resolved Entities\n",
    "\n",
    "Streams the complete entity report out of Senzing, requesting the raw source record JSON alongside each resolved entity.  We need the record-level JSON because that is where names, addresses, identifiers, and risk topics are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66421d8-37a3-48e6-8590-0d78a440c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Exported 150 entities...\n",
      "Exported 196 entities total\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "export_handle = sz_engine.export_json_entity_report(\n",
    "    flags=SzEngineFlags.SZ_EXPORT_INCLUDE_ALL_ENTITIES | \n",
    "          SzEngineFlags.SZ_ENTITY_INCLUDE_RECORD_JSON_DATA\n",
    ")\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    try:\n",
    "        entity_json = sz_engine.fetch_next(export_handle)\n",
    "        if not entity_json:\n",
    "            break\n",
    "        \n",
    "        entity = json.loads(entity_json)\n",
    "        entities.append(entity)\n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            print(f\"  Exported {count} entities...\", end='\\r')\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "sz_engine.close_export_report(export_handle)\n",
    "print(f\"\\nExported {len(entities)} entities total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adedc7a4-3d7d-4860-809b-f64c2eef39d4",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Summarizes what Senzing resolved: total records, unique entities, how many records were merged, and a breakdown by data source.  The cross-source resolution count (3 entities matched across OPEN-OWNERSHIP and OPEN-SANCTIONS) is the most analytically important number here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a1cbbd-90b7-4713-8840-4522f43d2de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "============================================================\n",
      "Total records in database: 282\n",
      "Total unique entities:     196\n",
      "Records merged:            86\n",
      "Reduction:                 30.5%\n",
      "\n",
      "Records by data source:\n",
      "  OPEN-SANCTIONS: 24\n",
      "  OPEN-OWNERSHIP: 258\n",
      "\n",
      "Cross-source resolutions: 3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Overview:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count total records from entities\n",
    "total_records = sum(len(e.get('RESOLVED_ENTITY', {}).get('RECORDS', [])) for e in entities)\n",
    "num_entities = len(entities)\n",
    "\n",
    "print(f\"Total records in database: {total_records:,}\")\n",
    "print(f\"Total unique entities:     {num_entities:,}\")\n",
    "print(f\"Records merged:            {total_records - num_entities:,}\")\n",
    "\n",
    "if total_records > 0:\n",
    "    print(f\"Reduction:                 {((total_records - num_entities) / total_records * 100):.1f}%\")\n",
    "\n",
    "# Count by data source\n",
    "from collections import Counter\n",
    "source_counts = Counter()\n",
    "for entity in entities:\n",
    "    records = entity.get('RESOLVED_ENTITY', {}).get('RECORDS', [])\n",
    "    for rec in records:\n",
    "        source = rec.get('DATA_SOURCE', 'UNKNOWN')\n",
    "        source_counts[source] += 1\n",
    "\n",
    "print(\"\\nRecords by data source:\")\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"  {source}: {count:,}\")\n",
    "\n",
    "# Count cross-source resolutions\n",
    "cross_source = 0\n",
    "for entity in entities:\n",
    "    records = entity.get('RESOLVED_ENTITY', {}).get('RECORDS', [])\n",
    "    sources = set(r.get('DATA_SOURCE') for r in records)\n",
    "    if len(sources) > 1:\n",
    "        cross_source += 1\n",
    "\n",
    "print(f\"\\nCross-source resolutions: {cross_source}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227ad9d-a365-4b72-ad82-f1944dcab3ec",
   "metadata": {},
   "source": [
    "## Build the Knowledge Graph with NetworkX\n",
    "\n",
    "Constructs the NetworkX graph from the exported entities.  Each node is a resolved entity carrying its name, type, record count, and data sources.  Edges represent business relationships (shareholding, directorship, etc.) extracted from the `RELATIONSHIPS` field in each source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52dc88af-f4b4-4c81-b5fe-cd5409453021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 196 entity nodes\n",
      "Added 396 relationship edges\n",
      "\n",
      "Knowledge Graph built:\n",
      "  Nodes: 196\n",
      "  Edges: 233\n",
      "  Connected components: 59\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# Add entities as nodes\n",
    "for entity in entities:\n",
    "    entity_data = entity.get('RESOLVED_ENTITY', {})\n",
    "    entity_id = entity_data.get('ENTITY_ID')\n",
    "    records = entity_data.get('RECORDS', [])\n",
    "    \n",
    "    if not records:\n",
    "        continue\n",
    "    \n",
    "    # Get entity info\n",
    "    first_record = records[0]\n",
    "    json_data = first_record.get('JSON_DATA', {})\n",
    "    record_type = json_data.get('RECORD_TYPE', 'UNKNOWN')\n",
    "    \n",
    "    # Get name\n",
    "    name = json_data.get('PRIMARY_NAME_FULL')\n",
    "    if not name:\n",
    "        name_list = json_data.get('NAMES', [])\n",
    "        for name_obj in name_list:\n",
    "            name = name_obj.get('NAME_FULL') or name_obj.get('PRIMARY_NAME_ORG') or name_obj.get('NAME_ORG')\n",
    "            if name:\n",
    "                break\n",
    "    \n",
    "    if not name:\n",
    "        name = f\"Entity {entity_id}\"\n",
    "    \n",
    "    # Get data sources\n",
    "    data_sources = list(set([r.get('DATA_SOURCE') for r in records]))\n",
    "    \n",
    "    # Add node\n",
    "    G.add_node(\n",
    "        entity_id,\n",
    "        name=name,\n",
    "        type=record_type,\n",
    "        num_records=len(records),\n",
    "        data_sources=data_sources\n",
    "    )\n",
    "\n",
    "print(f\"Added {G.number_of_nodes()} entity nodes\")\n",
    "\n",
    "# Add relationship edges\n",
    "edges_added = 0\n",
    "\n",
    "for entity in entities:\n",
    "    entity_data = entity.get('RESOLVED_ENTITY', {})\n",
    "    anchor_entity_id = entity_data.get('ENTITY_ID')\n",
    "    \n",
    "    # Look through records for relationships\n",
    "    for record in entity_data.get('RECORDS', []):\n",
    "        relationships = record.get('JSON_DATA', {}).get('RELATIONSHIPS', [])\n",
    "        \n",
    "        for rel in relationships:\n",
    "            pointer_key = rel.get('REL_POINTER_KEY')\n",
    "            pointer_role = rel.get('REL_POINTER_ROLE', 'related')\n",
    "            \n",
    "            # Find target entity\n",
    "            for target_entity in entities:\n",
    "                target_data = target_entity.get('RESOLVED_ENTITY', {})\n",
    "                target_entity_id = target_data.get('ENTITY_ID')\n",
    "                \n",
    "                for target_record in target_data.get('RECORDS', []):\n",
    "                    if target_record.get('RECORD_ID') == pointer_key:\n",
    "                        if anchor_entity_id != target_entity_id:\n",
    "                            G.add_edge(\n",
    "                                anchor_entity_id,\n",
    "                                target_entity_id,\n",
    "                                relationship=pointer_role\n",
    "                            )\n",
    "                            edges_added += 1\n",
    "                        break\n",
    "\n",
    "print(f\"Added {edges_added} relationship edges\")\n",
    "print(f\"\\nKnowledge Graph built:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Connected components: {nx.number_connected_components(G)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85d633-e416-4910-9aac-4fb688a152ce",
   "metadata": {},
   "source": [
    "## Load the Embedding Model\n",
    "\n",
    "Downloads and initializes `all-MiniLM-L6-v2` from Hugging Face.  This model produces 384-dimension embeddings and is a solid choice for this task since it is fast, small enough to run comfortably in the workshop environment, and handles the mix of names, addresses, and risk terminology in our entity text reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6b6507-f17f-438d-b66f-dd7168328815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477087048a84491d9d2e2521f6ff083b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded\n",
      "  Model: all-MiniLM-L6-v2\n",
      "  Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Embedding model loaded\")\n",
    "print(f\"  Model: all-MiniLM-L6-v2\")\n",
    "print(f\"  Embedding dimension: 384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444898a-ada4-4f28-ac61-28d35e854183",
   "metadata": {},
   "source": [
    "## Create Entity Embeddings\n",
    "\n",
    "Iterates over all entities and builds a text description for each one (name, type, data sources, address, identifiers, risk topics), then embeds it with the sentence transformer.  The text description is what gets stored in LanceDB and searched against at query time, so the fields included here directly determine what kinds of questions the RAG can answer well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864e06d9-5921-4d34-a3cc-7f3766376a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150 entities...\n",
      "Created embeddings for 196 entities\n"
     ]
    }
   ],
   "source": [
    "entity_data = []\n",
    "\n",
    "for entity in entities:\n",
    "    entity_data_item = entity.get('RESOLVED_ENTITY', {})\n",
    "    entity_id = entity_data_item.get('ENTITY_ID')\n",
    "    records = entity_data_item.get('RECORDS', [])\n",
    "    \n",
    "    if not records:\n",
    "        continue\n",
    "    \n",
    "    # Get entity info\n",
    "    first_record = records[0]\n",
    "    json_data = first_record.get('JSON_DATA', {})\n",
    "    \n",
    "    # Get name\n",
    "    name = json_data.get('PRIMARY_NAME_FULL')\n",
    "    if not name:\n",
    "        name_list = json_data.get('NAMES', [])\n",
    "        for name_obj in name_list:\n",
    "            name = name_obj.get('NAME_FULL') or name_obj.get('PRIMARY_NAME_ORG') or name_obj.get('NAME_ORG')\n",
    "            if name:\n",
    "                break\n",
    "    \n",
    "    if not name:\n",
    "        name = f\"Entity {entity_id}\"\n",
    "    \n",
    "    record_type = json_data.get('RECORD_TYPE', 'UNKNOWN')\n",
    "    data_sources = list(set([r.get('DATA_SOURCE') for r in records]))\n",
    "    \n",
    "    # Get addresses\n",
    "    addresses = []\n",
    "    for rec in records:\n",
    "        addrs = rec.get('JSON_DATA', {}).get('ADDRESSES', [])\n",
    "        for addr in addrs[:2]:\n",
    "            addr_full = addr.get('ADDR_FULL', '')\n",
    "            if addr_full:\n",
    "                addresses.append(addr_full)\n",
    "    \n",
    "    # Get identifiers\n",
    "    identifiers = []\n",
    "    for rec in records:\n",
    "        ids = rec.get('JSON_DATA', {}).get('IDENTIFIERS', [])\n",
    "        for id_obj in ids[:3]:\n",
    "            id_type = id_obj.get('NATIONAL_ID_TYPE') or id_obj.get('OTHER_ID_TYPE')\n",
    "            id_num = id_obj.get('NATIONAL_ID_NUMBER') or id_obj.get('OTHER_ID_NUMBER')\n",
    "            if id_type and id_num:\n",
    "                identifiers.append(f\"{id_type}: {id_num}\")\n",
    "    \n",
    "    # Get risks (from OPEN-SANCTIONS)\n",
    "    risks = []\n",
    "    for rec in records:\n",
    "        risk_list = rec.get('JSON_DATA', {}).get('RISKS', [])\n",
    "        for risk in risk_list:\n",
    "            topic = risk.get('TOPIC', '')\n",
    "            if topic:\n",
    "                risks.append(topic)\n",
    "    \n",
    "    # Create text description for embedding\n",
    "    text_parts = [\n",
    "        f\"Name: {name}\",\n",
    "        f\"Type: {record_type}\",\n",
    "        f\"Data sources: {', '.join(data_sources)}\",\n",
    "        f\"Records merged: {len(records)}\"\n",
    "    ]\n",
    "    \n",
    "    if addresses:\n",
    "        text_parts.append(f\"Address: {addresses[0]}\")\n",
    "    \n",
    "    if identifiers:\n",
    "        text_parts.append(f\"Identifiers: {', '.join(identifiers[:2])}\")\n",
    "    \n",
    "    if risks:\n",
    "        text_parts.append(f\"Risk topics: {', '.join(set(risks))}\")\n",
    "    \n",
    "    text = \". \".join(text_parts)\n",
    "    \n",
    "    # Create embedding\n",
    "    embedding = embedding_model.encode(text).tolist()\n",
    "    \n",
    "    # Store data\n",
    "    entity_data.append({\n",
    "        'entity_id': entity_id,\n",
    "        'name': name,\n",
    "        'type': record_type,\n",
    "        'text': text,\n",
    "        'vector': embedding,\n",
    "        'data_sources': ','.join(data_sources),\n",
    "        'num_records': len(records),\n",
    "        'addresses': '|'.join(addresses[:3]),\n",
    "        'identifiers': '|'.join(identifiers[:3]),\n",
    "        'risks': '|'.join(set(risks))\n",
    "    })\n",
    "    \n",
    "    if len(entity_data) % 50 == 0:\n",
    "        print(f\"  Processed {len(entity_data)} entities...\", end='\\r')\n",
    "\n",
    "print(f\"\\nCreated embeddings for {len(entity_data)} entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54cf8c-161b-471d-86ca-e53ce0a9fc52",
   "metadata": {},
   "source": [
    "## Store Embeddings in LanceDB\n",
    "\n",
    "Drops any existing `entities` table and writes all 196 entity records including their vectors into a fresh LanceDB table.  After this cell runs, the RAG notebook can connect to LanceDB and start querying without needing to touch Senzing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8945291c-afb7-4ffc-9343-80848bb2d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing table\n",
      "Creating new table...\n",
      "Stored 196 entities in LanceDB\n",
      "\n",
      "Data preparation complete!\n",
      "You can now use the RAG notebook to query this data.\n"
     ]
    }
   ],
   "source": [
    "db = lancedb.connect('/workspace/lancedb_data')\n",
    "\n",
    "# Drop existing table if it exists\n",
    "try:\n",
    "    db.drop_table('entities')\n",
    "    print(\"Dropped existing table\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new table\n",
    "print(\"Creating new table...\")\n",
    "table = db.create_table('entities', entity_data)\n",
    "\n",
    "print(f\"Stored {len(entity_data)} entities in LanceDB\")\n",
    "print(\"\\nData preparation complete!\")\n",
    "print(\"You can now use the RAG notebook to query this data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5f0b1-a7b6-4b96-9ff9-9caacc7a18de",
   "metadata": {},
   "source": [
    "## Preview LanceDB Contents\n",
    "\n",
    "Pulls the first 10 rows from the LanceDB table and displays the key metadata columns (entity ID, name, type, data sources, record count, risks).  This is a quick gut check to confirm the data looks right before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31a5faf-339b-4971-ba94-57864f9335a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanceDB Contents:\n",
      "======================================================================\n",
      "   entity_id                               name          type                   data_sources  num_records                risks\n",
      "0          1                    Abassin BADSHAH        PERSON  OPEN-OWNERSHIP,OPEN-SANCTIONS            3         corp.disqual\n",
      "1          2                      LMAR (GB) LTD  ORGANIZATION                 OPEN-SANCTIONS            1                     \n",
      "2          3            WANDLE HOLDINGS LIMITED  ORGANIZATION                 OPEN-SANCTIONS            1      sanction.linked\n",
      "3          4  POLYUS GOLD INTERNATIONAL LIMITED  ORGANIZATION                 OPEN-SANCTIONS            1      sanction.linked\n",
      "4          5          Firuza Nazimovna Kerimova        PERSON                 OPEN-SANCTIONS            1    role.rca|sanction\n",
      "5          6                     ООО \"ГРАНДЕКО\"  ORGANIZATION                 OPEN-SANCTIONS            2  sanction.linked|poi\n",
      "6          7        Amina Suleymanovna Kerimova        PERSON                 OPEN-SANCTIONS            1    role.rca|sanction\n",
      "7          8       Gulnara Suleimanova KERIMOVA        PERSON                 OPEN-SANCTIONS            1    role.rca|sanction\n",
      "8          9              BARLLOWS SERVICES LTD  ORGANIZATION                 OPEN-SANCTIONS            1                     \n",
      "9         10                       N. T. Wright        PERSON  OPEN-OWNERSHIP,OPEN-SANCTIONS            2             role.pep\n"
     ]
    }
   ],
   "source": [
    "print(\"LanceDB Contents:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample = table.to_pandas().head(10)\n",
    "display_columns = ['entity_id', 'name', 'type', 'data_sources', 'num_records', 'risks']\n",
    "print(sample[display_columns].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aded318-bc2a-4181-83db-4b08415614f3",
   "metadata": {},
   "source": [
    "## LanceDB Statistics\n",
    "\n",
    "Prints summary counts of what is in the vector store: total entities, breakdown by type (PERSON vs ORGANIZATION), breakdown by data source, and a count of entities that carry at least one risk flag.  Only 17 out of 196 entities have risk data, which reflects the relative size of the OPEN-SANCTIONS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc30c204-4acb-49f4-9ace-9488b8dd9e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LanceDB Statistics:\n",
      "======================================================================\n",
      "Total entities: 196\n",
      "\n",
      "By Type:\n",
      "type\n",
      "ORGANIZATION    127\n",
      "PERSON           69\n",
      "Name: count, dtype: int64\n",
      "\n",
      "By Data Source:\n",
      "data_sources\n",
      "OPEN-OWNERSHIP                   176\n",
      "OPEN-SANCTIONS                    17\n",
      "OPEN-OWNERSHIP,OPEN-SANCTIONS      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Entities with risks/sanctions:\n",
      "  Count: 17\n"
     ]
    }
   ],
   "source": [
    "all_entities = table.to_pandas()\n",
    "\n",
    "print(\"\\nLanceDB Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total entities: {len(all_entities)}\")\n",
    "print()\n",
    "\n",
    "print(\"By Type:\")\n",
    "print(all_entities['type'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"By Data Source:\")\n",
    "print(all_entities['data_sources'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Entities with risks/sanctions:\")\n",
    "has_risks = all_entities[all_entities['risks'].notna() & (all_entities['risks'] != '')]\n",
    "print(f\"  Count: {len(has_risks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafc3bd-0d6f-4841-94bd-09b85a7192f8",
   "metadata": {},
   "source": [
    "## Risk Topics Breakdown\n",
    "\n",
    "Lists every distinct risk topic in the dataset and shows example entities for each one.  This is useful context for the workshop because it tells participants exactly what risk vocabulary the chatbot knows about before they start asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435c59a6-91c7-4afa-baa5-53f4394952ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Topics in Dataset:\n",
      "======================================================================\n",
      "\n",
      "corp.disqual: 1 entities\n",
      "  Examples:\n",
      "    - Abassin BADSHAH (PERSON)\n",
      "\n",
      "poi: 2 entities\n",
      "  Examples:\n",
      "    - ООО \"ГРАНДЕКО\" (ORGANIZATION)\n",
      "    - Suleyman Abusaidovich KERIMOV (PERSON)\n",
      "\n",
      "role.oligarch: 1 entities\n",
      "  Examples:\n",
      "    - Suleyman Abusaidovich KERIMOV (PERSON)\n",
      "\n",
      "role.pep: 3 entities\n",
      "  Examples:\n",
      "    - N. T. Wright (PERSON)\n",
      "    - Mr Akhmet Magomedovich Palankoyev (PERSON)\n",
      "    - Suleyman Abusaidovich KERIMOV (PERSON)\n",
      "\n",
      "role.rca: 6 entities\n",
      "  Examples:\n",
      "    - Firuza Nazimovna Kerimova (PERSON)\n",
      "    - Amina Suleymanovna Kerimova (PERSON)\n",
      "    - Gulnara Suleimanova KERIMOVA (PERSON)\n",
      "\n",
      "sanction: 14 entities\n",
      "  Examples:\n",
      "    - WANDLE HOLDINGS LIMITED (ORGANIZATION)\n",
      "    - POLYUS GOLD INTERNATIONAL LIMITED (ORGANIZATION)\n",
      "    - Firuza Nazimovna Kerimova (PERSON)\n",
      "\n",
      "sanction.linked: 8 entities\n",
      "  Examples:\n",
      "    - WANDLE HOLDINGS LIMITED (ORGANIZATION)\n",
      "    - POLYUS GOLD INTERNATIONAL LIMITED (ORGANIZATION)\n",
      "    - ООО \"ГРАНДЕКО\" (ORGANIZATION)\n"
     ]
    }
   ],
   "source": [
    "all_entities = table.to_pandas()\n",
    "all_risks = all_entities[all_entities['risks'].notna() & (all_entities['risks'] != '')]\n",
    "\n",
    "print(\"Risk Topics in Dataset:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "risk_topics = set()\n",
    "for risks_str in all_risks['risks']:\n",
    "    if risks_str:\n",
    "        topics = risks_str.split('|')\n",
    "        risk_topics.update(topics)\n",
    "\n",
    "for topic in sorted(risk_topics):\n",
    "    count = sum(topic in str(r) for r in all_risks['risks'])\n",
    "    entities_with_risk = all_risks[all_risks['risks'].str.contains(topic, na=False)]\n",
    "    print(f\"\\n{topic}: {count} entities\")\n",
    "    print(\"  Examples:\")\n",
    "    for idx, row in entities_with_risk.head(3).iterrows():\n",
    "        print(f\"    - {row['name']} ({row['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedf9cc-550f-42a8-8a68-b40bae4bf9c3",
   "metadata": {},
   "source": [
    "## Inspect Sample Entity Records\n",
    "\n",
    "Prints the full stored record for the first 5 entities including the first and last few values of each embedding vector.  This gives participants a concrete look at exactly what shape of data is going into LanceDB and being searched at query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f83c40-7317-46f7-8547-7767c883090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity 1:\n",
      "======================================================================\n",
      "{\n",
      "  \"entity_id\": 1,\n",
      "  \"name\": \"Abassin BADSHAH\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"text\": \"Name: Abassin BADSHAH. Type: PERSON. Data sources: OPEN-OWNERSHIP, OPEN-SANCTIONS. Records merged: 3. Address: 31 Quernmore Close, Bromley, Kent, United Kingdom, BR1 4EL. Identifiers: OPEN-SANCTIONS: NK-25vyVFzt8vdJGgAXMRTwTJ. Risk topics: corp.disqual\",\n",
      "  \"data_sources\": \"OPEN-OWNERSHIP,OPEN-SANCTIONS\",\n",
      "  \"num_records\": 3,\n",
      "  \"addresses\": \"31 Quernmore Close, Bromley, Kent, United Kingdom, BR1 4EL|31, Quernmore Close, Bromley, BR1 4EL|3, Market Parade, 41 East Street, Bromley, BR1 1QN\",\n",
      "  \"identifiers\": \"OPEN-SANCTIONS: NK-25vyVFzt8vdJGgAXMRTwTJ\",\n",
      "  \"risks\": \"corp.disqual\"\n",
      "}\n",
      "\n",
      "vector: [0.008808483369648457, -0.023793436586856842, 0.00644419901072979, ..., -0.034974053502082825]\n",
      "  (384-dimension vector, first few values shown)\n",
      "\n",
      "\n",
      "Entity 2:\n",
      "======================================================================\n",
      "{\n",
      "  \"entity_id\": 2,\n",
      "  \"name\": \"LMAR (GB) LTD\",\n",
      "  \"type\": \"ORGANIZATION\",\n",
      "  \"text\": \"Name: LMAR (GB) LTD. Type: ORGANIZATION. Data sources: OPEN-SANCTIONS. Records merged: 1. Address: 31 Quernmore Close, Bromley, Kent, United Kingdom, BR1 4EL. Identifiers: OPEN-SANCTIONS: NK-3p3mmVWmjwVtTfKchz4kNE\",\n",
      "  \"data_sources\": \"OPEN-SANCTIONS\",\n",
      "  \"num_records\": 1,\n",
      "  \"addresses\": \"31 Quernmore Close, Bromley, Kent, United Kingdom, BR1 4EL\",\n",
      "  \"identifiers\": \"OPEN-SANCTIONS: NK-3p3mmVWmjwVtTfKchz4kNE\",\n",
      "  \"risks\": \"\"\n",
      "}\n",
      "\n",
      "vector: [0.014756930060684681, -0.05207272991538048, 0.00835822056978941, ..., -0.0042417761869728565]\n",
      "  (384-dimension vector, first few values shown)\n",
      "\n",
      "\n",
      "Entity 3:\n",
      "======================================================================\n",
      "{\n",
      "  \"entity_id\": 3,\n",
      "  \"name\": \"WANDLE HOLDINGS LIMITED\",\n",
      "  \"type\": \"ORGANIZATION\",\n",
      "  \"text\": \"Name: WANDLE HOLDINGS LIMITED. Type: ORGANIZATION. Data sources: OPEN-SANCTIONS. Records merged: 1. Address: DEANA BEACH APTS, BLOCK A, Flat 212, \\u03a0\\u03c1\\u03bf\\u03bc\\u03b1\\u03c7\\u03c9\\u0301\\u03bd \\u0395\\u03bb\\u03b5\\u03c5\\u03b8\\u03b5\\u03c1\\u03b9\\u0301\\u03b1\\u03c2, 33, '\\u0391\\u03b3\\u03b9\\u03bf\\u03c2 \\u0391\\u03b8\\u03b1\\u03bd\\u03b1\\u0301\\u03c3\\u03b9\\u03bf\\u03c2, 4103, \\u039b\\u03b5\\u03bc\\u03b5\\u03c3\\u03bf\\u0301\\u03c2, \\u039a\\u03c5\\u0301\\u03c0\\u03c1\\u03bf\\u03c2. Identifiers: OPEN-SANCTIONS: NK-auyPsLrBzRoxjCRWgjBvas. Risk topics: sanction.linked\",\n",
      "  \"data_sources\": \"OPEN-SANCTIONS\",\n",
      "  \"num_records\": 1,\n",
      "  \"addresses\": \"DEANA BEACH APTS, BLOCK A, Flat 212, \\u03a0\\u03c1\\u03bf\\u03bc\\u03b1\\u03c7\\u03c9\\u0301\\u03bd \\u0395\\u03bb\\u03b5\\u03c5\\u03b8\\u03b5\\u03c1\\u03b9\\u0301\\u03b1\\u03c2, 33, '\\u0391\\u03b3\\u03b9\\u03bf\\u03c2 \\u0391\\u03b8\\u03b1\\u03bd\\u03b1\\u0301\\u03c3\\u03b9\\u03bf\\u03c2, 4103, \\u039b\\u03b5\\u03bc\\u03b5\\u03c3\\u03bf\\u0301\\u03c2, \\u039a\\u03c5\\u0301\\u03c0\\u03c1\\u03bf\\u03c2\",\n",
      "  \"identifiers\": \"OPEN-SANCTIONS: NK-auyPsLrBzRoxjCRWgjBvas\",\n",
      "  \"risks\": \"sanction.linked\"\n",
      "}\n",
      "\n",
      "vector: [-0.004003793932497501, 0.0016979649662971497, 0.004017823841422796, ..., -0.0056622265838086605]\n",
      "  (384-dimension vector, first few values shown)\n",
      "\n",
      "\n",
      "Entity 4:\n",
      "======================================================================\n",
      "{\n",
      "  \"entity_id\": 4,\n",
      "  \"name\": \"POLYUS GOLD INTERNATIONAL LIMITED\",\n",
      "  \"type\": \"ORGANIZATION\",\n",
      "  \"text\": \"Name: POLYUS GOLD INTERNATIONAL LIMITED. Type: ORGANIZATION. Data sources: OPEN-SANCTIONS. Records merged: 1. Address: 3RD FLOOR CHARTER PLACE 23-27 SEATON PALCE, ST HELIER JE4 0WH. Identifiers: OPEN-SANCTIONS: NK-cf4Q3KcmUnQbt8Cy7iTtwK. Risk topics: sanction.linked\",\n",
      "  \"data_sources\": \"OPEN-SANCTIONS\",\n",
      "  \"num_records\": 1,\n",
      "  \"addresses\": \"3RD FLOOR CHARTER PLACE 23-27 SEATON PALCE, ST HELIER JE4 0WH\",\n",
      "  \"identifiers\": \"OPEN-SANCTIONS: NK-cf4Q3KcmUnQbt8Cy7iTtwK\",\n",
      "  \"risks\": \"sanction.linked\"\n",
      "}\n",
      "\n",
      "vector: [0.037982407957315445, 0.008133403956890106, -0.014482112601399422, ..., -0.043614719063043594]\n",
      "  (384-dimension vector, first few values shown)\n",
      "\n",
      "\n",
      "Entity 5:\n",
      "======================================================================\n",
      "{\n",
      "  \"entity_id\": 5,\n",
      "  \"name\": \"Firuza Nazimovna Kerimova\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"text\": \"Name: Firuza Nazimovna Kerimova. Type: PERSON. Data sources: OPEN-SANCTIONS. Records merged: 1. Address: MOSCOW, RUS, 123430. Risk topics: role.rca, sanction\",\n",
      "  \"data_sources\": \"OPEN-SANCTIONS\",\n",
      "  \"num_records\": 1,\n",
      "  \"addresses\": \"MOSCOW, RUS, 123430|Apt. 270, Build. 31, Pyatnitskoe Shosse, 123430 Moscow\",\n",
      "  \"identifiers\": \"\",\n",
      "  \"risks\": \"role.rca|sanction\"\n",
      "}\n",
      "\n",
      "vector: [-0.0008964235894382, 0.056125808507204056, -0.11759068816900253, ..., -0.035333335399627686]\n",
      "  (384-dimension vector, first few values shown)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = table.to_pandas().head(5)\n",
    "\n",
    "# Convert to dict and display as formatted JSON\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"\\nEntity {idx + 1}:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Convert row to dict\n",
    "    row_dict = row.to_dict()\n",
    "    \n",
    "    # Show vector info separately\n",
    "    vector = row_dict.pop('vector')\n",
    "    \n",
    "    # Print everything except vector\n",
    "    print(json.dumps(row_dict, indent=2))\n",
    "    \n",
    "    # Print vector summary\n",
    "    print(f\"\\nvector: [{vector[0]}, {vector[1]}, {vector[2]}, ..., {vector[-1]}]\")\n",
    "    print(f\"  (384-dimension vector, first few values shown)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f4583e-952d-48e5-a685-5ca25898bbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing connections...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Closing connections...\")\n",
    "\n",
    "try:\n",
    "    grpc_channel.close()\n",
    "    print(\"Done\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff216861-0241-494e-94cb-142f5def54c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
